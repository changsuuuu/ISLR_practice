{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y=f(x)+e 에서 f를 추정하는 이유\n",
    "1. prediction\n",
    "    \n",
    "2. inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear models allow for relatively simple and inter- pretable inference, but may not yield as accurate predictions as some other approaches. In contrast, some of the highly non-linear approaches that we discuss in the later chapters of this book can potentially provide quite accu- rate predictions for Y , but this comes at the expense of a less interpretable model for which inference is more challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 Measuring the Quality of Fit\n",
    "\n",
    "-MSE\n",
    "\n",
    "    computed using the training data that was used to fit the model\n",
    "    cross-validation (Chapter 5), which is a method for estimating test MSE using the training data\n",
    "\n",
    "    there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE.\n",
    "\n",
    "    degrees of freedom, for a number of smoothing splines. The de- grees of freedom is a quantity that summarizes the flexibility of a curve\n",
    "\n",
    "    cross-validation (Chapter 5), which is a method for estimating test MSE using the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 The Bias-Variance Trade-Off\n",
    "\n",
    "    in order to minimize the expected test error,\n",
    "    we need to select a statistical learning method that simultaneously achieves low variance and low bias.\n",
    "\n",
    "    Variance refers to the amount by which fˆ would change if we estimated it using a different training data set.\n",
    "\n",
    "    bias refers to the error that is introduced by approxi- mating a real-life problem, which may be extremely complicated, by a much simpler model.\n",
    "\n",
    "    As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease.\n",
    "\n",
    "    As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases.\n",
    "\n",
    "\n",
    "    However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.3 The Classification Setting\n",
    "\n",
    "    [The Bayes Classifier]\n",
    "\n",
    "        It is possible to show that the test error rate given in (2.9) is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values.\n",
    "        \n",
    "        The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate.\n",
    "\n",
    "        The Bayes error rate is analogous to the irreducible error\n",
    "\n",
    "    [K-Nearest Neighbors]\n",
    "\n",
    "        In theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distri- bution of Y given X, and so computing the Bayes classifier is impossible.\n",
    "\n",
    "        Many approaches attempt to estimate the conditional distribution of Y given X, and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier.\n",
    "\n",
    "        Despite the fact that it is a very simple approach, KNN can often pro- duce classifiers that are surprisingly close to the optimal Bayes classifier.\n",
    "\n",
    "        The choice of K has a drastic effect on the KNN classifier obtained.\n",
    "\n",
    "        As K grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.\n",
    "\n",
    "        In general, as we use more flexible classification methods, the training error rate will decline but the test error rate may not. \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
