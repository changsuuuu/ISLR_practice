{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we discuss some widely-used classifiers: logistic regression, linear discriminant analysis, quadratic dis- criminant analysis, naive Bayes, and K-nearest neighbors. The discussion of logistic regression is used as a jumping-off point for a discussion of gen- eralized linear models, and in particular, Poisson regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are at least two reasons not to perform classifica- tion using a regression method: \n",
    "\n",
    "(a) a regression method cannot accommo- date a qualitative response with more than two classes; \n",
    "\n",
    "(b) a regression method will not provide meaningful estimates of Pr(Y |X), even with just two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀 fit 방법 : mle\n",
    "\n",
    "회귀계수해석 : x의 한 단위 증가는 y의 log odds 의 beta 배 변화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 로지스틱을 돌렸을 때, 단순 로지스틱과 부호가 반대로 나오는 경우도 있는데, 다른 변수들이 고정된 상태(알려진 상태)에서는 그러한 결과가 나올 수 있다. 공선성이 대표적인 예 인듯? 따라서, 이미 그 효과가 고정되어 있어서 학생인지 아닌지만 봤을 때는 학생인 경우가 더 default위험이 적을 수 있다. 하지만 아무런 정보가 없으면, 학생이 더 default위험이 더 높다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the linear regression setting, the results obtained using one predictor may be quite different from those ob- tained using multiple predictors, especially when there is correlation among the predictors. >> confounding(교란변수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multinomial regression : more than two classes in Y\n",
    "\n",
    "interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline.\n",
    "\n",
    "The softmax coding is equivalent softmax to the coding just described in the sense that the fitted values, log odds between any pair of classes, and other key model outputs will remain the\n",
    "same, regardless of coding.\n",
    "\n",
    "rather than estimating coefficients for K − 1 classes, we actually estimate coefficients for all K classes because of (4.13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 Generative Models for classification\n",
    "\n",
    "use Bayes’ theorem to flip these around into estimates for Pr(Y = k|X = x). When the distribution of X within each class is assumed to be normal, it turns out that the model is very similar in form to logistic regression.\n",
    "\n",
    "로지스틱 대신 이것이 필요한 이유\n",
    "\n",
    "    1. When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable.\n",
    "\n",
    "    2. If the distribution of the predictors X is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression. (데이터가 정규분포를 따르고, 표본크기가 작으면 로지스틱보다 좋은 듯)\n",
    "\n",
    "    3. Y의 범주가 3개 이상인 경우에 적용가능\n",
    "\n",
    "(4.15)에서, f_k(x)를 추정하는 것이 어렵다. >> to estimate fk(x), we will typically have to make some simplifying assumptions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.1 Linear Discriminant Analysis for p=1 (one predictor)\n",
    "\n",
    "(assumption)\n",
    "\n",
    "    (Bayes Classifier)\n",
    "    1. f_k(x) : normal or gaussian\n",
    "    2. 등분산가정  >> 이 가정이 포함된 pdf를 (4.15)에 넣자\n",
    "\n",
    "하지만 여기서도 mu와 sigma를 추정하는 것은 어려움\n",
    "\n",
    "Bayes Classifier를 approximate 하는게 LDA \n",
    "    표본평균, 표본분산, 어떤 정보가 없다면 상대도수(prior)를 통해서 근사\n",
    "\n",
    "4.4.2 Linear Discriminant Analysis for p>1\n",
    "\n",
    "(assumption)\n",
    "    design matrix is drawn from a multivariate normal dist.\n",
    "\n",
    "δk(x) is a linear function of x; that is, the LDA decision rule depends on x only through a linear combination of its elements.\n",
    "\n",
    "(결과를 볼 때 주의점)\n",
    "\n",
    "1. training error rates will usually be lower than test error\n",
    "rates, which are the real quantity of interest. overfitting과 관련\n",
    "2. the trivial null classifier will achieve an error rate that null is only a bit higher than the LDA training set error rate. (null classifier 가 error rate는 가장 작다는 뜻?)\n",
    "\n",
    "(confusion matrix)\n",
    "\n",
    "sensitivity is the percentage of true defaulters that are identified\n",
    "\n",
    "specificity is the percentage of non-defaulters that are correctly identified\n",
    "\n",
    "적절한 threshold 를 잡는 것은 trade off 때문에 도메인 지식이 중요\n",
    "\n",
    "( ROC : Receiver Operating Characteristics )\n",
    "\n",
    "the larger the AUC the better the classifier.\n",
    "\n",
    "4.4.3 Quadratic Discriminant Analysis\n",
    "\n",
    "Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to per- form prediction. owever, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form X ∼ N(μk,Σk)\n",
    "\n",
    "LDA 와 QDA 중 어느 하나가 선호되는 경우? >> bias-variance trade-off \n",
    "\n",
    "    QDA 는 kp(p+1)/2 만큼 모수를 추정해야하는데, LDA는 common variance 가정 덕분에 Kp개로 줄어듬. 즉, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. This can potentially lead to improved prediction performance. But there is a trade-off\n",
    "    \n",
    "    >> LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable.\n",
    "\n",
    "4.4.4 Naive Bayes\n",
    "\n",
    "앞에서의 DA 들의 여러 가정(정규성 가정 등) 대신 . 'Within the kth class, the p predictors are independent.' 라는 가정 하나만 가정한다. Stated mathematically, this assumption means that for k = 1, . . . , K ,\n",
    "fk(x) = fk1(x1) × fk2(x2) × · · · × fkp(xp), where fkj is the density function of the jth predictor among observations in the kth class. >> covariates 들의 joint dist. 를 구하지 않아도 됨.\n",
    "물론 이 가정이 맞다고 믿을 수 는 없지만, it often leads to pretty decent results, especially in settings where n is not large enough relative to p for us to effectively estimate the joint distribution of the predic- tors within each class. \n",
    "\n",
    "Essentially, the naive Bayes assumption introduces some bias, but reduces variance, leading to a classifier that works quite well in practice as a result of the bias-variance trade-off. 즉, QDA 에서 covariance matrix 가 diagonal 이라는 것과 같음.\n",
    "\n",
    "(f_kj 를 추정하는 방법)\n",
    "\n",
    "1. xj가 양적변수이면, 비모수방법을 쓸 수 있음 >> very simple way to do this is by making a his- togram for the observations of the jth predictor within each class. Then we can estimate fkj(xj) as the fraction of the training observations in the kth class that belong to the same histogram bin as xj. Alternatively, we can use a kernel density estimator, which is essen- tially a smoothed version of a histogram.\n",
    "\n",
    "2. 질적변수이면, we can simply count the proportion of train- ing observations for the jth predictor corresponding to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 A Comparison of Classification Methods\n",
    "\n",
    "4.5.1 An Analytical Comparison\n",
    "\n",
    "comparison of \" LDA, QDA, naive Bayes, and logistic regression. \"\n",
    "\n",
    "- LDA는 logistic regression과 마찬가지로 사후확률의 로그오즈가 x에 대한 선형\n",
    "\n",
    "- QDA는 사후확률의 로그오즈가 x에 대한 quadratic    \n",
    "\n",
    "- Naive Bayes 는 사후확률의 로그오즈가 generalized addtive model 의 형태\n",
    "\n",
    "- Neither QDA nor naive Bayes is a special case of the other. Naive Bayes can produce a more flexible fit, since any choice can be made for gkj(xj). However, it is restricted to a purely additive fit, in the sense that in (4.34), a function of xj is added to a function of xl, for j ̸= l; however, these terms are never multiplied. By contrast, QDA includes multiplicative terms of the form ckjlxjxl. Therefore, QDA has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes.\n",
    "\n",
    "- LDA와 multinomial logistic reg. 은 선형형태가 동일하지만, 가정이 다름. we expect LDA to outperform logistic regression when the normality assumption (approxi- mately) holds, and we expect logistic regression to perform better when it does not.\n",
    "\n",
    "\n",
    "(KNN 과의 차이?)\n",
    "KNN is a completely non-parametric approach: no assumptions are made about the shape of the decision boundary.\n",
    "\n",
    "- because KNN is completely non-parametric, we can expect this ap- proach to dominate LDA and logistic regression when the decision boundary is highly non-linear, provided that n is very large and p is small.\n",
    "\n",
    "- KNN 은 많은 데이터가 필요(n>>p) 왜냐하면, knn 은 비모수 모델이기 때문에 분산을 크게 하면서 편차를 작게 하기 때문\n",
    "\n",
    "- decision boundary가 non-linear 이고 n은 적당한데 p가 작지 않으면, QDA가 KNN보다 선호된다. 왜냐하면 QDA는 parametric form의 장점을 제공한다. 즉, 정확한 분류를 위해서, knn보다 더 적은 표본크기를 필요로 함.\n",
    "\n",
    "- knn 은 logistic regression과 다르게, 어떤 변수가 중요한지를 알려주지 않음.\n",
    "\n",
    "4.5.2 An Empirical Comparison\n",
    "\n",
    "LDA : normal ((decision boundary가 non-linear(due to the unequal cov. matrix) 이면 별로))\n",
    "QDA : normal + correlation\n",
    "logistic : (decision boundary가 non-linear(due to the unequal cov. matrix) 이면 별로)\n",
    "Naive Bayes : independent (n 이 작으면 QDA 보다 좋음)\n",
    "KNN-CV : non-linear? (more flexible)\n",
    "KNN-1 : if non-linear , non-parametric method such as knn gives poor results\n",
    "\n",
    "정리하면, \n",
    "    true decision boundaries are linear >> LDA & logistic good\n",
    "    true decision boundaries are non-linear >> QDA & Naive Bayes  good\n",
    "    much more complicated decision boundaries >> KNN good\n",
    "    이러한 이유때문에, the level of smoothness for a non-parametric approach must be chosen carefully\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
