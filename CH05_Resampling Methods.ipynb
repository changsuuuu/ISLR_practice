{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Resampling Methods (다시 읽어보자)\n",
    "\n",
    "- cross-valiation / bootstrap\n",
    "\n",
    "    cv : can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.\n",
    "\n",
    "    bootstrap : provide a measure of accuracy of a parameter estimate or of a given statistical learning method.\n",
    "\n",
    "    * model assesment : process of evaluating a model’s performance\n",
    "    * model selection : the process ofs electing the proper level of flexibility for a model\n",
    "\n",
    "5.1 Cross Validation\n",
    "\n",
    "5.1.1 The Validation Set Approach(반으로 나눠서 하는 느낌)\n",
    "\n",
    "randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set.\n",
    "\n",
    "- two potential drawbacks\n",
    "    \n",
    "    1. the validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set. (나눌 때 마다 다르다는 의미인듯)\n",
    "\n",
    "    2. 전체 데이터 중 일부만 가지고 fit 하므로, 데이터가 작으면 test error rate 를 overestimate 하는 경우가 생길 수 있음.\n",
    "\n",
    "5.1.2 Leave-One-Out Cross-Validation\n",
    "\n",
    "single observation (x1,y1) is used for the validation set, and the remaining observations {(x2, y2), . . . , (xn, yn)} make up the training set.\n",
    "\n",
    "(5.1.1) 방법과는 다른 장점\n",
    " - less bias \n",
    " - 위 방법은 매 번 다른 결과가 나올 수 있는 반면, cv는 항상 같은 결과를 내줌. 즉, traing set과 validation set 에 randomness가 없음\n",
    "\n",
    "5.1.3 k-Fold Cross Validation (다시읽어보자)\n",
    "\n",
    "randomly dividing the set of observations into k groups, or folds, of approximately equal size.\n",
    "\n",
    "loocv 보다는 computatinally intensive\n",
    "\n",
    "despite the fact that they sometimes underestimate the true test MSE, all of the CV curves come close to identifying the correct level of flexibility. that is, the flexibility level corresponding to the smallest test MSE.\n",
    "\n",
    "5.1.4 Bias-Variance Trade off for k-fold cross validation\n",
    "\n",
    "k-fold 방법이 loocv방법보다 computaion 측면에서 장점을 가지지만, 더 중요한 장점은 k-fold는 loocv보다 test error rate를 더 정확히 추정하는 경우가 많다.\n",
    "\n",
    "the validation set approach can lead to overestimates of the test error rate, since in this approach the training set used to fit the statistical learning method contains only half the observations of the entire data set.\n",
    "\n",
    "from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k-fold CV.\n",
    "however, we must also consider the procedure’s variance.\n",
    "\n",
    "It turns out that LOOCV has higher variance than does k-fold CV with k < n. 왜냐하면, LOOCV는 n 개의 적합모델의 아웃푹을 n으로 나누기 때문에, 아웃풋들이 서로 거의 동일해서, 서로 높게 correlated 되어있다. 반면에 K fold 는 다소 덜 correlated 되어있다 (because training set 들이 덜 correlated 되어있기때문.)\n",
    "\n",
    "Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.\n",
    "\n",
    "5.1.5 cross validation on classification problems (걍 ch05 다시 읽자,,)\n",
    "\n",
    "양적변수에서는 mse, 질적변수에서는 misclassified 된 관측치들의 개수를 사용한다.\n",
    "\n",
    "5.2 The Bootstrap(랜덤 샘플링을 통해 training set을 늘리는 방법으로 이해하면 될 듯)\n",
    "\n",
    "The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given esti- mator or statistical learning method.\n",
    "\n",
    "As a simple example, the bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit.\n",
    "\n",
    "the bootstrap approach allows us to use a computer to emulate the process of obtaining new sample sets, so that we can estimate the variability of αˆ without generating additional samples.\n",
    "\n",
    "we obtain distinct data sets by repeatedly sampling observations from the original data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
