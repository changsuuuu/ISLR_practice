{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\n",
    "\n",
    "before moving to the non-linear world, we discuss in this chapter some ways in which the simple linear model can be improved, by replacing plain least squares fitting with some alternative fitting procedures.\n",
    "\n",
    "최소제곱법 대신에 다른 fitting 절차를 쓰는 이유?\n",
    "\n",
    "1. 더 좋은 예측 정확성\n",
    "\n",
    "if n<p,  By constraining or shrinking the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias. This can lead to substantial improvements in the accuracy with which we can predict the response for observations not used in model training.\n",
    "\n",
    "2. 모델의 해석\n",
    "\n",
    "관련없는 변수들을 제거함으로써 더 쉽게 해석할 수 있다. In this chapter, we see some approaches for au- tomaticallyperformingfeatureselectionorvariableselection—thatis, for excluding irrelevant variables from a multiple regression model.\n",
    "\n",
    "우리는 subset selection, shrinkage, dimension reduction 을 다룰 예정\n",
    "\n",
    "6.1 Subset Selection\n",
    "\n",
    "6.1.1 best subset selection( 2^p 개 해보는거인듯)\n",
    "\n",
    " In the case of logistic regression, instead of ordering models by RSS in Step 2 of Algorithm 6.1, we instead use the deviance, a measure that plays the role of RSS for a broader class of models.\n",
    "\n",
    "best subset selection becomes computationally infeasible for values of p greater than around 40, even with extremely fast modern computers.\n",
    "\n",
    "6.1.2 stepwise selection\n",
    "\n",
    "- forward\n",
    "    Though forward stepwise tends to do well in practice, it is not guaranteed to find the best possible model out of all 2p mod- els containing subsets of the p predictors.\n",
    "\n",
    "    Forward stepwise selection can be applied even in the high-dimensional setting where n < p; however, in this case, it is possible to construct sub- models M0, . . . , Mn−1 only, since each submodel is fit using least squares, which will not yield a unique solution if p ≥ n.\n",
    "\n",
    "- backward \n",
    "    Backward selection requires that the number of samples n is larger than the number of variables p (so that the full model can be fit). In contrast, forward stepwise can be used even when n < p, and so is the only viable subset method when p is very large.\n",
    "\n",
    "- hybrid approaches\n",
    "\n",
    "6.1.3 choosing the optimal model\n",
    "\n",
    "RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.\n",
    "In order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:\n",
    "\n",
    "    1. We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\n",
    "    \n",
    "    2. We can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in Chapter 5.\n",
    "\n",
    "We show in Chapter 2 that the training set MSE is generally an under- estimate of the test MSE. This is because when we fit a model to the training data using least squares, we specifi- cally estimate the regression coefficients such that the training RSS (but not the test RSS) is as small as possible. In particular, the training error will decrease as more variables are included in the model, but the test error may not. Therefore, training set RSS and training set R2 cannot be used to select from among a set of models with different numbers of variables.\n",
    "\n",
    "- Cp, AIC, BIC adj R^2(얘는 높은게 좋음)\n",
    "\n",
    "    Cp statistic adds a penalty of 2dσˆ2 to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error.\n",
    "\n",
    "    AIC criterion is defined for a large class of models fit by maximum likelihood.(다중회귀에서 정규에러일때, mle와 lse는 같음)\n",
    "\n",
    "    for least squares models, Cp and AIC are proportional to each other\n",
    "\n",
    "    BIC is derived from a Bayesian point of view, but ends up looking similar to Cp (and AIC) as well.\n",
    "\n",
    "    for any n > 7, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than Cp\n",
    "\n",
    "    Despite its pop- ularity, and even though it is quite intuitive, the adjusted R2 is not as well motivated in statistical theory as AIC, BIC, and Cp. we have presented their formulas in the case of a linear model fit using least squares; however, AIC and BIC can also be defined for more general types of models.\n",
    "\n",
    "- validation and cross validation\n",
    "\n",
    "    This procedure has an advantage relative to AIC, BIC, Cp, and adjusted R2, in that it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model. It can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g. the number of predictors in the model) or hard to estimate the error variance σ2.\n",
    "\n",
    "    cross- validation is a very attractive approach for selecting from among a number of models under consideration.\n",
    "\n",
    "    if we repeated cross-validation using a different set of cross-validation folds, then the precise model with the lowest estimated test error would surely change. In this setting, we can select a model using the one-standard-error rule. We first calculate the one standard error of the estimated test MSE for each model size, and then standard- select the smallest model for which the estimated test error is within one error standard error of the lowest point on the curve.\n",
    "\n",
    "6.2 shrinkage methods\n",
    "\n",
    "it turns out that shrinking the coefficient estimates can significantly reduce their variance.\n",
    "\n",
    "6.2.1 ridge regression\n",
    "\n",
    "the shrinkage penalty is applied to β1,...,βp, but not to the intercept β0\n",
    "\n",
    "the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant. 하지만, 최소제곱법은 그대로임\n",
    "\n",
    "In other words,\n",
    "Xj βˆR will depend not only on the value of λ, but also on the scaling of the j,λ\n",
    "jth predictor.\n",
    "\n",
    "Therefore, it is best to apply ridge regression after standardizing the predictors\n",
    "\n",
    "As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias\n",
    "\n",
    "the least squares estimates will have low bias but may have high variance. This means that a small change in the training data can cause a large change in the least squares coefficient estimates.\n",
    "\n",
    "if p > n, then the least squares estimates do not even have a unique solution, whereas ridge regression can still perform well by trading off a small increase in bias for a large decrease in variance. \n",
    "\n",
    "6.2.2 lasso\n",
    "\n",
    "릿지는 forward backward stepwise 와는 달리 최종 모델에 p개의 예측변수가 모두 포함된다는 것이 단점이다.\n",
    "\n",
    "it will not set any of them exactly to zero (unless λ = ∞) (0에 가깝게 계수를 추정)\n",
    "이는 모델의 정확성 측면에서는 문제가 없지만, p가 클 때, 해석의 측면에서는 어려움을 자아낸다. >> 라쏘는 이 단점을 극복함\n",
    "\n",
    "the l1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large. >> 변수선택의 효과를 줌\n",
    "\n",
    "models generated from the lasso are generally much easier to interpret than those produced\n",
    "by ridge regression. >> sparse model을 제공\n",
    "\n",
    "figure 6.6을 보면 릿지와는 달리 라쏘는 하나씩 추가되는 느낌 (릿지는 첨부터 다 들어있음)\n",
    "\n",
    "식 6.10은 best subset selection이지만 p가 크면 계산적으로 어렵다.\n",
    "\n",
    "- comparing the lasso and ridge regression\n",
    "\n",
    "The lasso implicitly assumes that a number of the coefficients truly equal zero. Consequently, it is not surprising that ridge regression outperform\n",
    "\n",
    "neither ridge regression nor the lasso will universally dominate the other.\n",
    "\n",
    "A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.\n",
    "\n",
    "- A Simple Special Case for Ridge Regression and the Lasso\n",
    "\n",
    "In ridge regression, each least squares coefficient estimate is shrunken by the same proportion\n",
    "\n",
    "In contrast, the lasso shrinks each least squares coefficient towards zero by a constant amount, λ/2; the least squares coefficients that are less than λ/2 in absolute value are shrunken entirely to zero.\n",
    "\n",
    "ridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount, and sufficiently small co- efficients are shrunken all the way to zero.\n",
    "\n",
    "- Bayesian Interpretation for Ridge Regression and the Lasso\n",
    "\n",
    "If g is a Gaussian distribution with mean zero and standard deviation\n",
    "a function of λ, then it follows that the posterior mode for β is given by the ridge mode regression solution\n",
    "\n",
    "If g is a double-exponential (Laplace) distribution with mean zero and scale parameter a function of λ, then it follows that the posterior mode for β is the lasso solution. \n",
    "\n",
    "6.2.3 selecting the tuning parameter\n",
    "\n",
    "Cross-validation provides a sim- ple way to tackle this problem\n",
    "\n",
    "We choose a grid of λ values, and compute the cross-validation error for each value of λ\n",
    "\n",
    "We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter\n",
    "\n",
    "6.3 dimension reduction methods\n",
    "\n",
    "Dimension reduction serves to constrain the estimated βj coefficients (식 6.18)\n",
    "\n",
    "This constraint on the form of the coefficients has the potential to bias the coefficient estimates\n",
    "\n",
    "If M = p, and all the Zm are linearly independent, then (6.18) poses no constraints. In this case, no dimension reduction occurs >> LSE와 같아짐\n",
    "\n",
    "pca & parital least squares\n",
    "\n",
    "6.3.1 principal components regression\n",
    "\n",
    "It is necessary to consider only linear combinations of the form φ11^2 + φ21^2 = 1, since otherwise we could increase φ11 and φ21 arbitrarily in order to blow up the variance.\n",
    "\n",
    "we assume that the directions in which X1, . . . , Xp show the most variation are the directions that are associated with Y . While this assumption is not guaranteed to be true, it often turns out to be a reasonable enough approximation to give good results.\n",
    "\n",
    "PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response.\n",
    "\n",
    "even though PCR provides a simple way to perform regression using M < p predictors, it is not a feature selection method\n",
    "\n",
    "In this sense, PCR is more closely related to ridge regression than to the lasso.\n",
    "\n",
    "One can even think of ridge regression as a continuous version of PCR\n",
    "\n",
    "6.3.2 partial least squares\n",
    "\n",
    "These directions(PCR) are identified in an unsupervised way, since the response Y is not used to help determine the principal component directions. \n",
    "\n",
    "PCR suffers from a drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response\n",
    "\n",
    "PLS(partial~) 얘는 supervised learning. p->M개의 선형결합으로 줄이고, 이를 최소제곱법으로 fitting하는 것은 동일\n",
    "\n",
    "But unlike PCR, PLS identifies these new features in a supervised way. that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response\n",
    "\n",
    "Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.\n",
    "\n",
    "첫번째 PLS direction을 계산하는 방법 : After standardizing the p predictors, PLS computes the first direction Z1 by setting each φj1 in (6.16) equal to the coefficient from the simple linear regression of Y onto Xj. One can show that this coefficient is proportional to the correlation between Y and Xj. Hence, in computing Z1 = pj=1 φj1Xj, PLS places the highest weight on the variables that are most strongly related to the response.\n",
    "\n",
    "PLS direction does not fit the predictors as closely as does PCA, but it does a better job explaining the response.\n",
    "\n",
    "to identify the second PLS direction we first adjust each of the variables for Z1, by regressing each variable on Z1 and taking residuals\n",
    "\n",
    "6.4\n",
    "\n",
    "6.4.1 high dimensional data\n",
    "\n",
    "원래는 p>>>n 인 경우지만, 여기서는 n>p(약간 작을 때)일때 supervised learning 측면에서 보자\n",
    "\n",
    "6.4.2 what goes wrong in high dimensions?\n",
    "\n",
    "though it is possible to perfectly fit the training data in the high-dimensional setting, the resulting linear model will perform extremely poorly on an independent test set, and therefore does not constitute a useful model\n",
    "\n",
    "Unfortunately, the Cp, AIC, and BIC approaches are not appropriate in the high-dimensional setting, because estimating σˆ2 is problematic. Similarly, problems arise in the application of adjusted R2 in the high-dimensional setting, since one can easily obtain a model with an adjusted R2 value of 1. Clearly, alternative approaches that are better-suited to the high-dimensional setting are required.\n",
    "\n",
    "6.4.3 regression in high dimensions\n",
    "\n",
    "It turns out that many of the methods seen in this chapter for fitting less flexible least squares models, such as forward stepwise selection, ridge regression, the lasso, and principal components regression, are particularly useful for performing regression in the high-dimensional setting. Essentially, these approaches avoid overfitting by using a less flexible fitting approach than least squares.\n",
    "\n",
    "collection of measurements for thousands or millions of features can lead to improved predictive models if these features are in fact relevant to the problem at hand, but will lead to worse results if the features are not relevant\n",
    "\n",
    "6.4.4 interpreting results in high dimensions\n",
    "\n",
    "In the high-dimensional setting, the multicollinearity problem is extreme: \n",
    "\n",
    "one should never use sum of squared errors, p-values, R2 statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
